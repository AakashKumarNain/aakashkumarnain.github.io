---
title: "Normalized Transformer"
subtitle: ""
author: "Aakash Kumar Nain ([@A_K_Nain](https://x.com/A_K_Nain))"
date: "2024-10-23"
categories: [papers, summary, transformers, research]
image: ""
format:
  html:
    theme: default
    fontsize: 2em
    backgroundcolor: rgb(255, 255, 255);
    code-fold: true
    code-summary: "Show the code"
    highlight-style: oblivion
execute: 
  echo: false
---

We have been using transformers daily for a couple of years now, but the advancements on the architectural side have remained minimal. This paper from Nvidia proposes a **normalized Transformer (nGPT)**, which performs representation learning on a hypersphere. Here is a summary for the same:


# Token embeddings and output logits

1. Both input and output embedding matrices are normalized after each training step.
2. The logits are bounded in the range [-1, 1] because of the normalization, which limits the confidence of the probability distribution generated by the softmax.
3. To adjust this during training, the authors introduce a trainable scaling parameter *sz* that scales the logits element-wise.



# Layers and Blocks

1. A typical transformer block looks like this where L layers of transformations are applied to the hidden state h, consisting of alternating the self-attention and MLP blocks:<br><br>
![Typical Transformer Block](paper_screenshots/nGPT/1.png)

<br><br>
2. If we are on a hypersphere, we can use spherical linear interpolation to compute the geodesic distance between two points a and b on the hypersphere. We can approximate SLERP with approximate linear interpolation (LERP) as shown below: <br><br>
![SLERP and LERP](paper_screenshots/nGPT/2.png)

<br><br>
3. If point a is our hidden state h, and point b represents the point suggested by the attention or MLP block, we can represent our equations for the transformer blocks like this: <br><br>
![Representing hidden state and attention or MLP outputs on hypersphere](paper_screenshots/nGPT/3.png)

<br><br>
4. Here α~A~ ≥0 and α~M~ ≥0, with dimensionality d~model~, are learnable parameters applied to the normalized outputs of the attention and MLP blocks, respectively. The norm function normalizes any vector x to have a unit norm and, unlike RMSNorm or LayerNorm, does not introduce any element-wise scaling factors. This normalization can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold.