[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog posts",
    "section": "",
    "text": "Blog posts\n\nNormalized Transformer\nNormalized Transformer: Cooked by Nvidia!\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/nGPT.html",
    "href": "posts/nGPT.html",
    "title": "Normalized Transformer",
    "section": "",
    "text": "We have been using transformers daily for a couple of years now, but the advancements on the architectural side have remained minimal. This paper from Nvidia proposes a normalized Transformer (nGPT), which performs representation learning on a hypersphere. Here is a summary for the same:\n\nToken embeddings and output logits\n\nBoth input and output embedding matrices are normalized after each training step.\nThe logits are bounded in the range [-1, 1] because of the normalization, which limits the confidence of the probability distribution generated by the softmax.\nTo adjust this during training, the authors introduce a trainable scaling parameter sz that scales the logits element-wise.\n\n\n\nLayers and Blocks\n\nA typical transformer block looks like this where L layers of transformations are applied to the hidden state h, consisting of alternating the self-attention and MLP blocks: \n\n 2. If we are on a hypersphere, we can use spherical linear interpolation to compute the geodesic distance between two points a and b on the hypersphere. We can approximate SLERP with approximate linear interpolation (LERP) as shown below:  \n 3. If point a is our hidden state h, and point b represents the point suggested by the attention or MLP block, we can represent our equations for the transformer blocks like this:  \n 4. Here αA ≥0 and αM ≥0, with dimensionality dmodel, are learnable parameters applied to the normalized outputs of the attention and MLP blocks, respectively. The norm function normalizes any vector x to have a unit norm and, unlike RMSNorm or LayerNorm, does not introduce any element-wise scaling factors. This normalization can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold.\n\n\nSelf-Attention Blocks\n\nThe qkv values produced by the weight matrics Wq, Wk, and Wv in the original transformer are unconstrained, leading to unbounded values in q.\nIn nGPT the authors normalize Wq, Wk, Wv and Wo along their embedding dimension so that the computed dot products with h can be interpreted as cosine similarity between unit norm vectors bounded in [−1, 1]. Thus, all attention matrices can be viewed as collections of normalized embedding vectors to be compared.\nThough each element of q and k is now bounded, the norms of these two vectors can still vary. Also, the addition of positional embeddings can further distort q and k. To this end, the authors additionally normalize q and k by introducing a scaling factor sqk for each head, ensuring that the dot product of every query and key is under control.\nIn the original Transformer, the query-key dot product is scaled by 1/√d_k before applying softmax to account for the expected variance of dk in the dot product of non-normalized query and key vectors. In the normalized Transformer, the expected variance of the dot product between the normalized query and key vectors is 1/dk. The softmax scaling factor should instead be √d_k to restore a variance of 1\n\n\n\n\nMLP\n\nThe input hidden state h of the MLP block of a classical transformer is first normalized using RMSNorm(or LayerNorm) and then passed through two separate linear projections, producing two intermediate vectors u and v, which are then combined using SwiGLU.\nThe weight matrices Wu and Wv in nGPT are normalized. The authors introduce scaling factors su and sν to control their impact. They also rescale ν by √d_model to optimize SiLU performance.\n\n\n\n\nSummary of all modifications\n\nRemove all normalization layers like RMSNorm or LayerNorm.\nAfter each training step, normalize all matrices (Einput, Eoutput, Wq, Wk, Wv, Wo, Wu, Wν, and Wo) along their embedding dimension.\nReplace the updates as follows where αA (and also αM) is treated with αA,init = 0.05 (in order of 1/n_layers) and αA,scale = 1/√d_model.\nChange the softmax scaling factor in attention from 1/√d_k to √d_k.\nImplement the rescaling and normalization of q and k where sqk is treated with sqk,init = 1 and sqk,scale = 1/√d_model.\nImplement the rescaling of the intermediate state of the MLP block where su (and also sν) is treated with su,init = 1 and su,scale = 1\nImplement the rescaling of logits using equation 3, where sz is treated with sz,init = 1, sz,scale = 1/√d_model.\nRemove weight decay and learning rate warmup."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Hello, I am Aakash Nain.\nI am a Machine Learning Engineer with 7 years of experience in the industry. Currently, I am working as a Senior ML Engineer at Merlyn Mind. I am also a Google Developers Expert in Machine Learning and JAX. I actively contribute to Keras, TensorFlow, and the JAX ecosystem. I am one of the core collaborators for the Keras core project, and I am also one of the maintainers of the TensorFlow-addons package.\nIn terms of work, my knowledge in Data Science and Machine Learning spans in T-shape. I have worked in multiple domains, but Computer Vision with Deep Learning is my favorite field. I love simplifying Data Science and Machine Learning concepts, especially the maths behind them. Love and support OSS because, without OSS, we wouldn’t have witnessed so many breakthroughs in Machine Learning and Deep Learning. Python is my favorite language to work with, and I absolutely love coding in it. Always up for a discussion involving anything related to Machine Learning, Deep Learning, MLOps, and API design. Simply put: Python in breakfast, Machine Learning in lunch, and Deep Learning in dinner!\nFollow me on Twitter to get the latest updates related to Machine Learning."
  },
  {
    "objectID": "index.html#popular-projects",
    "href": "index.html#popular-projects",
    "title": "About me",
    "section": "Popular Projects",
    "text": "Popular Projects\nAnnotated Research Papers   \nMost researchers and Machine Learning engineers want to remain up-to-date with the latest research work, but the field of machine learning moves crazy fast. The number of papers uploaded on arXiv is witnessing exponential growth. Reading all the papers that come out daily is an impossible task. To help with this, I maintain this repository where I annotate and upload the papers that I find exceptionally good. Annotations make it easier to understand and grasp the main concepts explained in the paper. \nTF-JAX Tutorials   \nIt is a series of tutorials built to teach the fundamental concepts and the underlying working of two famous libraries: TensorFlow and JAX. These tutorials aren’t typical documentation-type tutorials. It doesn’t matter whether you are a beginner or an advanced user, these tutorials will give you a fresh perspective on building things using TensorFlow or JAX.\nDiffusion Models   \nDiffusion models are a class of likelihood-based generative models that recently have been used to produce very high-quality images compared to other existing generative models like GANs. It’s hard to find quality resources to learn about diffusion models. The mathematics behind the diffusion models is also a bit harder to understand. The material presented in this repo is enough to make you understand the working of diffusion models and the underlying math."
  },
  {
    "objectID": "index.html#keras-core-contributions",
    "href": "index.html#keras-core-contributions",
    "title": "About me",
    "section": "Keras Core Contributions",
    "text": "Keras Core Contributions\n\nJAX NN ops\nMerging layers\nMetrics\nLoss functions\nApplications\nData adapters\nCode examples"
  },
  {
    "objectID": "index.html#keras-contributions",
    "href": "index.html#keras-contributions",
    "title": "About me",
    "section": "Keras Contributions",
    "text": "Keras Contributions\n\nDenoising Diffusion Models\nOCR model for reading captchas\nHandwriting recognition\nImage captioning\nWGAN-GP\nCycleGAN\nModel interpretability with Integrated Gradients"
  },
  {
    "objectID": "index.html#equinox-contributions",
    "href": "index.html#equinox-contributions",
    "title": "About me",
    "section": "Equinox Contributions",
    "text": "Equinox Contributions\n\nMistral-7B implementation\nBug fix in RMSNorm Layer\nAMP in Normalization Layers"
  },
  {
    "objectID": "index.html#tensorflow-contributions",
    "href": "index.html#tensorflow-contributions",
    "title": "About me",
    "section": "TensorFlow Contributions",
    "text": "TensorFlow Contributions\n\nKappa\nGelu activation\nFocal loss\nThresholded Linear Unit"
  }
]